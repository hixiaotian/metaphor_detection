{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>pos_seq</th>\n",
       "      <th>label_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Ca n't fail to be entertaining .</td>\n",
       "      <td>['VERB', 'ADV', 'VERB', 'PART', 'VERB', 'ADJ',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>How much was he going to tell her ?</td>\n",
       "      <td>['ADV', 'ADJ', 'VERB', 'PRON', 'VERB', 'PART',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Up until that news hit the Committee , Don had...</td>\n",
       "      <td>['ADP', 'ADP', 'DET', 'NOUN', 'VERB', 'DET', '...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Could go on to the rugby and go with them coul...</td>\n",
       "      <td>['VERB', 'VERB', 'PART', 'ADP', 'DET', 'NOUN',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Finally , we went to the office and they gave ...</td>\n",
       "      <td>['ADV', 'PUNCT', 'PRON', 'VERB', 'ADP', 'DET',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6318</td>\n",
       "      <td>In a voice of soft persuasion , she said , Wil...</td>\n",
       "      <td>['ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', '...</td>\n",
       "      <td>[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6319</td>\n",
       "      <td>It is a symptom of public anxiety about urban ...</td>\n",
       "      <td>['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', ...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6320</td>\n",
       "      <td>I do n't like Miss Fitch .</td>\n",
       "      <td>['PRON', 'VERB', 'ADV', 'VERB', 'PROPN', 'PROP...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6321</td>\n",
       "      <td>A fern-like plant , beautifully preserved in a...</td>\n",
       "      <td>['DET', 'ADJ', 'NOUN', 'PUNCT', 'ADV', 'VERB',...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6322</td>\n",
       "      <td>And there were never more than a few dozen rin...</td>\n",
       "      <td>['CCONJ', 'ADV', 'VERB', 'ADV', 'ADJ', 'ADP', ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6323 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  \\\n",
       "0                      Ca n't fail to be entertaining .   \n",
       "1                   How much was he going to tell her ?   \n",
       "2     Up until that news hit the Committee , Don had...   \n",
       "3     Could go on to the rugby and go with them coul...   \n",
       "4     Finally , we went to the office and they gave ...   \n",
       "...                                                 ...   \n",
       "6318  In a voice of soft persuasion , she said , Wil...   \n",
       "6319  It is a symptom of public anxiety about urban ...   \n",
       "6320                         I do n't like Miss Fitch .   \n",
       "6321  A fern-like plant , beautifully preserved in a...   \n",
       "6322  And there were never more than a few dozen rin...   \n",
       "\n",
       "                                                pos_seq  \\\n",
       "0     ['VERB', 'ADV', 'VERB', 'PART', 'VERB', 'ADJ',...   \n",
       "1     ['ADV', 'ADJ', 'VERB', 'PRON', 'VERB', 'PART',...   \n",
       "2     ['ADP', 'ADP', 'DET', 'NOUN', 'VERB', 'DET', '...   \n",
       "3     ['VERB', 'VERB', 'PART', 'ADP', 'DET', 'NOUN',...   \n",
       "4     ['ADV', 'PUNCT', 'PRON', 'VERB', 'ADP', 'DET',...   \n",
       "...                                                 ...   \n",
       "6318  ['ADP', 'DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', '...   \n",
       "6319  ['PRON', 'VERB', 'DET', 'NOUN', 'ADP', 'ADJ', ...   \n",
       "6320  ['PRON', 'VERB', 'ADV', 'VERB', 'PROPN', 'PROP...   \n",
       "6321  ['DET', 'ADJ', 'NOUN', 'PUNCT', 'ADV', 'VERB',...   \n",
       "6322  ['CCONJ', 'ADV', 'VERB', 'ADV', 'ADJ', 'ADP', ...   \n",
       "\n",
       "                                              label_seq  \n",
       "0                                 [0, 0, 0, 0, 0, 0, 0]  \n",
       "1                           [0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2     [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "3            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "6318  [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6319  [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6320                              [0, 0, 0, 0, 0, 0, 0]  \n",
       "6321                  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "6322  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[6323 rows x 3 columns]"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('./data_release/train.csv', encoding='latin-1')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unused code, probably OK to get rid of at some point\n",
    "\n",
    "# def preprocess(sentence):\n",
    "#     word_pattern = re.compile(\"(\\w+|<s> |[,.!?;\\(\\)])\")\n",
    "#     return word_pattern.findall(sentence)\n",
    "\n",
    "#     def prob_tagged_sequence(self, sequence):\n",
    "#         '''\n",
    "#         sequence: tuple where first element is sentence as a string, second element is a \n",
    "#             list of tags associated with each word of sentence \n",
    "            \n",
    "#         tags are binary (0 or 1)\n",
    "#         '''\n",
    "#         sentence = sequence[0].lower().split()\n",
    "#         tags = sequence[1]\n",
    "        \n",
    "#         #initialize log_prob_acc with initial probability\n",
    "#         initial_transition_prob = self.tag_bigrams[('<START>', tags[0])] / self.tag_counts['<START>']\n",
    "#         if sentence[0] in self.emissions:\n",
    "#             initial_emission_prob = self.emissions[sentence[0]].get(tags[0], 1) / self.tag_counts[tags[0]]\n",
    "#         else:\n",
    "#             initial_emission_prob = 1 / self.tag_counts[tags[0]]\n",
    "#         log_prob_acc = math.log(initial_transition_prob) + math.log(initial_emission_prob)\n",
    "        \n",
    "#         #sum log transition and emission probabilities\n",
    "#         for t in range(1, len(tags)):\n",
    "#             tag_bigram = (tags[t-1], tags[t])\n",
    "#             transition_prob = self.tag_bigrams[tag_bigram] / self.tag_counts[tags[t-1]]\n",
    "#             if sentence[t] in self.emissions:\n",
    "#                 emission_prob = self.emissions[sentence[t]].get(tags[t], 1) / self.tag_counts[tags[t]]\n",
    "#             else:\n",
    "#                 emission_prob = 1 / self.tag_counts[tags[t]]\n",
    "#             log_prob_acc = math.log(transition_prob) + math.log(emission_prob)\n",
    "        \n",
    "#         return math.exp(log_prob_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM_Metaphor_Tagger():\n",
    "    def __init__(self, training_data, k=0.5, weights={(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}):\n",
    "        self.tag_counts = Counter()\n",
    "        self.tag_bigrams = {}\n",
    "        self.emissions = {}\n",
    "        \n",
    "        #set k for add-k smoothing\n",
    "        self.k = k\n",
    "        \n",
    "        #set weights for transition probabilities (NOT CURRENTLY SET)\n",
    "        self.weights = weights\n",
    "        \n",
    "        #iterate through training examples\n",
    "        for row in training_data.iterrows():\n",
    "            \n",
    "            #preprocess: add start characters and labels for computing initial probabilities\n",
    "            # and convert strings to lists and downcase sentences\n",
    "            tags_string = row[1][2]\n",
    "            tags = ast.literal_eval(tags_string)\n",
    "            tags.insert(0, '<START>')\n",
    "            sentence = row[1][0].lower().split()\n",
    "            sentence.insert(0, '<s>')\n",
    "            \n",
    "            #get label bigram counts -- (0,0), (0,1), (1,0), (1,1), ('<START>',0), ('<START>',1)\n",
    "            for t in range(1, len(tags)):\n",
    "                tag_bigram = (tags[t-1], tags[t])\n",
    "                if tag_bigram not in self.tag_bigrams:\n",
    "                    self.tag_bigrams[tag_bigram] = 1\n",
    "                else:\n",
    "                    self.tag_bigrams[tag_bigram] += 1\n",
    "                    \n",
    "            #get individual tag counts\n",
    "            self.tag_counts.update(tags)\n",
    "            \n",
    "            #get emission counts\n",
    "            for i, word in enumerate(sentence):\n",
    "                if word not in self.emissions:\n",
    "                    self.emissions[word] = {tags[i] : 1}\n",
    "                else:\n",
    "                    if tags[i] not in self.emissions[word]:\n",
    "                        self.emissions[word][tags[i]] = 1\n",
    "                    else:\n",
    "                        self.emissions[word][tags[i]] += 1\n",
    "    \n",
    "    \n",
    "    def viterbi(self, sentence):\n",
    "        '''\n",
    "        sentence: string where each token (punctuation included) is separated by a space\n",
    "        '''\n",
    "        sentence = sentence.lower().split()\n",
    "        previous_log_scores = []\n",
    "        backpointers = []\n",
    "        tags = list(self.tag_counts)\n",
    "\n",
    "        #initialization\n",
    "        for t in range(1, len(tags)):\n",
    "            tag = tags[t]\n",
    "\n",
    "            initial_transition_prob = self.tag_bigrams[('<START>', tag)] / self.tag_counts['<START>']\n",
    "            if sentence[0] in self.emissions:\n",
    "                initial_emission_prob = self.emissions[sentence[0]].get(tag, self.k) / self.tag_counts[tag]\n",
    "            else:\n",
    "                initial_emission_prob = self.k / self.tag_counts[tag]\n",
    "            \n",
    "            previous_log_scores.append(math.log(initial_transition_prob) + math.log(initial_emission_prob))\n",
    "        \n",
    "        #iteration\n",
    "        #w is index of current word\n",
    "        for w in range(1, len(sentence)):\n",
    "            \n",
    "            log_scores = [None, None]\n",
    "            w_backpointers = []\n",
    "            max_log_score_final = (float('-inf'), None)\n",
    "            \n",
    "            #t is index of current tag\n",
    "            for t in range(1, len(tags)):\n",
    "                \n",
    "                t_backpointer = None\n",
    "                max_log_score = (float('-inf'), None)\n",
    "\n",
    "                #j is index of previous tag\n",
    "                for j in range(1, len(tags)):\n",
    "                    \n",
    "                    transition_prob = self.tag_bigrams[(tags[j], tags[t])] / self.tag_counts[tags[j]]\n",
    "                    if sentence[w] in self.emissions:\n",
    "                        emission_prob = self.emissions[sentence[w]].get(tags[t], self.k) / self.tag_counts[tags[t]]\n",
    "                    else:\n",
    "                        emission_prob = self.k / self.tag_counts[tags[t]]\n",
    "                    \n",
    "                    weight = self.weights[(tags[j], tags[t])]\n",
    "                    log_score = previous_log_scores[j-1] + weight * math.log(transition_prob) + math.log(emission_prob)\n",
    "                    if log_score > max_log_score[0]:\n",
    "                        max_log_score = (log_score, j)\n",
    "                        t_backpointer = j\n",
    "                        \n",
    "                    if max_log_score[0] > max_log_score_final[0]:\n",
    "                        max_log_score_final = max_log_score\n",
    "                \n",
    "                log_scores[t-1] = max_log_score[0]\n",
    "                w_backpointers.append(t_backpointer)\n",
    "                \n",
    "            previous_log_scores = log_scores\n",
    "            backpointers.insert(0, w_backpointers)\n",
    "        \n",
    "        #backtracking\n",
    "        max_index = previous_log_scores.index(max(previous_log_scores)) + 1\n",
    "        output = [tags[max_index]]\n",
    "    \n",
    "        if len(sentence) == 1:\n",
    "            return output\n",
    "        \n",
    "        max_index = max_log_score_final[1]\n",
    "        for bptrs in backpointers:\n",
    "            max_index = bptrs[max_index-1]\n",
    "            output.insert(0, tags[max_index])\n",
    "            \n",
    "        return output\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv('./data_release/val.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_data):\n",
    "    labels = []\n",
    "    for row in val_data.iterrows():\n",
    "        sentence = row[1][0]\n",
    "        labels += model.viterbi(sentence)\n",
    "    ids = [i for i in range(len(labels))]\n",
    "    df = pd.DataFrame({'idx': ids, 'label': labels}, columns = ['idx', 'label'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HMM_Metaphor_Tagger(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.viterbi('he continued , hackles rising . he is drowning in debt .'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: k = 0.25, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}, comparable accuracy, comparable F1\n",
    "#2: k = 0.5, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}, treating as baseline\n",
    "#3: k = 1, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, worse F1\n",
    "#4: k = 2, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, worse F1\n",
    "\n",
    "#5: k = 10, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, worse F1\n",
    "#6: k = 100, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, worse F1\n",
    "#7: k = 1000, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, worse F1\n",
    "\n",
    "#max F1 at k = 0.5\n",
    "\n",
    "#8: k = 0.5, weights = {(0,0): 1, (0,1): 2, (1,0): 2, (1,1): 4}, comparable accuracy, worse F1\n",
    "#9: k = 0.5, weights = {(0,0): 4, (0,1): 2, (1,0): 2, (1,1): 1}, comparable accuracy, worse F1\n",
    "#10: k = 0.5, weights = {(0,0): 2, (0,1): 1, (1,0): 1, (1,1): 1}, slightly worse accuracy, slightly better F1\n",
    "#11: k = 0.5, weights = {(0,0): 4, (0,1): 1, (1,0): 1, (1,1): 1}, slightly worse accuracy, slightly better F1\n",
    "#12: k = 0.5, weights = {(0,0): 10, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, worse F1\n",
    "\n",
    "#13: k = 0.5, weights = {(0,0): 5, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, highest F1\n",
    "\n",
    "#14: k = 0.5, weights = {(0,0): 6, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, worse F1\n",
    "#15: k = 0.5, weights = {(0,0): 5, (0,1): 2, (1,0): 2, (1,1): 1}, comparable accuracy, worse F1\n",
    "#15: k = 0.5, weights = {(0,0): 5, (0,1): 2, (1,0): 2, (1,1): 1}, comparable accuracy, worse F1\n",
    "#16: k = 0.5, weights = {(0,0): 5, (0,1): 0.5, (1,0): 0.5, (1,1): 0.5}, comparable accuracy, worse F1\n",
    "\n",
    "#max F1 at k = 0.5 and weights = {(0,0): 5, (0,1): 1, (1,0): 1, (1,1): 1} (model 13)\n",
    "\n",
    "#analysis: the above max parameters result in a very slightly higher F1 score than the baseline\n",
    "# but with low precision and high recall and as such I believe the baseline (model 2, AKA unweighted) \n",
    "# is in fact the most robust model since it has precision roughly equal to recall roughly equal to F1\n",
    "# as well as the highest accuracy across all tests\n",
    "\n",
    "#final model parameters: k = 0.5, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1} (model 2)\n",
    "\n",
    "df = validate(model, val_data)\n",
    "df.to_csv('model_1_validation_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./data_release/test_no_label.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data):\n",
    "    labels = []\n",
    "    for row in test_data.iterrows():\n",
    "        sentence = row[1][0]\n",
    "        labels += model.viterbi(sentence)\n",
    "    ids = [i for i in range(1, len(labels)+1)]\n",
    "    df = pd.DataFrame({'idx': ids, 'label': labels}, columns = ['idx', 'label'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test(model, test_data)\n",
    "df.to_csv('model_1_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
