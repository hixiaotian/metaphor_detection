{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data_release/train.csv', encoding='latin-1')\n",
    "val_data = pd.read_csv('./data_release/val.csv', encoding='latin-1')\n",
    "test_data = pd.read_csv('./data_release/test_no_label.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM_Metaphor_Tagger():\n",
    "    def __init__(self, training_data, k=0.5, weights={(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}):\n",
    "        self.tag_counts = Counter()\n",
    "        self.tag_bigrams = {}\n",
    "        self.emissions = {}\n",
    "        \n",
    "        #set k for add-k smoothing\n",
    "        self.k = k\n",
    "        \n",
    "        #set weights for transition probabilities\n",
    "        self.weights = weights\n",
    "        \n",
    "        #iterate through training examples\n",
    "        for row in training_data.iterrows():\n",
    "            #preprocess: add start characters and labels for computing initial probabilities\n",
    "            # and convert strings to lists and downcase sentences\n",
    "            tags_string = row[1][2]\n",
    "            tags = ast.literal_eval(tags_string)\n",
    "            tags.insert(0, '<START>')\n",
    "            sentence = row[1][0].lower().split()\n",
    "            sentence.insert(0, '<s>')\n",
    "            \n",
    "            #get label bigram counts -- (0,0), (0,1), (1,0), (1,1), ('<START>',0), ('<START>',1)\n",
    "            for t in range(1, len(tags)):\n",
    "                tag_bigram = (tags[t-1], tags[t])\n",
    "                if tag_bigram not in self.tag_bigrams:\n",
    "                    self.tag_bigrams[tag_bigram] = 1\n",
    "                else:\n",
    "                    self.tag_bigrams[tag_bigram] += 1\n",
    "                    \n",
    "            #get individual tag counts\n",
    "            self.tag_counts.update(tags)\n",
    "            \n",
    "            #get emission counts\n",
    "            for i, word in enumerate(sentence):\n",
    "                if word not in self.emissions:\n",
    "                    self.emissions[word] = {tags[i] : 1}\n",
    "                else:\n",
    "                    if tags[i] not in self.emissions[word]:\n",
    "                        self.emissions[word][tags[i]] = 1\n",
    "                    else:\n",
    "                        self.emissions[word][tags[i]] += 1\n",
    "    \n",
    "    \n",
    "    def viterbi(self, sentence):\n",
    "        '''\n",
    "        sentence: string where each token (punctuation included) is separated by a space\n",
    "        '''\n",
    "        sentence = sentence.lower().split()\n",
    "        previous_log_scores = []\n",
    "        backpointers = []\n",
    "        tags = list(self.tag_counts)\n",
    "\n",
    "        #initialization\n",
    "        for t in range(1, len(tags)):\n",
    "            tag = tags[t]\n",
    "\n",
    "            initial_transition_prob = self.tag_bigrams[('<START>', tag)] / self.tag_counts['<START>']\n",
    "            if sentence[0] in self.emissions:\n",
    "                initial_emission_prob = self.emissions[sentence[0]].get(tag, self.k) / self.tag_counts[tag]\n",
    "            else:\n",
    "                initial_emission_prob = self.k / self.tag_counts[tag]\n",
    "            \n",
    "            previous_log_scores.append(math.log(initial_transition_prob) + math.log(initial_emission_prob))\n",
    "        \n",
    "        #iteration\n",
    "        #w is index of current word\n",
    "        for w in range(1, len(sentence)):\n",
    "            \n",
    "            log_scores = [None, None]\n",
    "            w_backpointers = []\n",
    "            max_log_score_final = (float('-inf'), None)\n",
    "            \n",
    "            #t is index of current tag\n",
    "            for t in range(1, len(tags)):\n",
    "                \n",
    "                t_backpointer = None\n",
    "                max_log_score = (float('-inf'), None)\n",
    "\n",
    "                #j is index of previous tag\n",
    "                for j in range(1, len(tags)):\n",
    "                    \n",
    "                    transition_prob = self.tag_bigrams[(tags[j], tags[t])] / self.tag_counts[tags[j]]\n",
    "                    if sentence[w] in self.emissions:\n",
    "                        emission_prob = self.emissions[sentence[w]].get(tags[t], self.k) / self.tag_counts[tags[t]]\n",
    "                    else:\n",
    "                        emission_prob = self.k / self.tag_counts[tags[t]]\n",
    "                    \n",
    "                    weight = self.weights[(tags[j], tags[t])]\n",
    "                    log_score = previous_log_scores[j-1] + weight * math.log(transition_prob) + math.log(emission_prob)\n",
    "                    if log_score > max_log_score[0]:\n",
    "                        max_log_score = (log_score, j)\n",
    "                        t_backpointer = j\n",
    "                        \n",
    "                    if max_log_score[0] > max_log_score_final[0]:\n",
    "                        max_log_score_final = max_log_score\n",
    "                \n",
    "                log_scores[t-1] = max_log_score[0]\n",
    "                w_backpointers.append(t_backpointer)\n",
    "                \n",
    "            previous_log_scores = log_scores\n",
    "            backpointers.insert(0, w_backpointers)\n",
    "        \n",
    "        #backtracking\n",
    "        max_index = previous_log_scores.index(max(previous_log_scores)) + 1\n",
    "        output = [tags[max_index]]\n",
    "    \n",
    "        if len(sentence) == 1:\n",
    "            return output\n",
    "        \n",
    "        max_index = max_log_score_final[1]\n",
    "        for bptrs in backpointers:\n",
    "            max_index = bptrs[max_index-1]\n",
    "            output.insert(0, tags[max_index])\n",
    "            \n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: k = 0.25, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}, comparable accuracy, comparable F1\n",
    "#2: k = 0.5, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}, treating as baseline\n",
    "#3: k = 1, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, worse F1\n",
    "#4: k = 2, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, worse F1\n",
    "\n",
    "#5: k = 10, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, worse F1\n",
    "#6: k = 100, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, worse F1\n",
    "#7: k = 1000, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, worse F1\n",
    "\n",
    "#max F1 at k = 0.5\n",
    "\n",
    "#8: k = 0.5, weights = {(0,0): 1, (0,1): 2, (1,0): 2, (1,1): 4}, comparable accuracy, worse F1\n",
    "#9: k = 0.5, weights = {(0,0): 4, (0,1): 2, (1,0): 2, (1,1): 1}, comparable accuracy, worse F1\n",
    "#10: k = 0.5, weights = {(0,0): 2, (0,1): 1, (1,0): 1, (1,1): 1}, slightly worse accuracy, slightly better F1\n",
    "#11: k = 0.5, weights = {(0,0): 4, (0,1): 1, (1,0): 1, (1,1): 1}, slightly worse accuracy, slightly better F1\n",
    "#12: k = 0.5, weights = {(0,0): 10, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, worse F1\n",
    "\n",
    "#13: k = 0.5, weights = {(0,0): 5, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, highest F1\n",
    "\n",
    "#14: k = 0.5, weights = {(0,0): 6, (0,1): 1, (1,0): 1, (1,1): 1}, worse accuracy, worse F1\n",
    "#15: k = 0.5, weights = {(0,0): 5, (0,1): 2, (1,0): 2, (1,1): 1}, comparable accuracy, worse F1\n",
    "#15: k = 0.5, weights = {(0,0): 5, (0,1): 2, (1,0): 2, (1,1): 1}, comparable accuracy, worse F1\n",
    "#16: k = 0.5, weights = {(0,0): 5, (0,1): 0.5, (1,0): 0.5, (1,1): 0.5}, comparable accuracy, worse F1\n",
    "\n",
    "#max F1 at k = 0.5 and weights = {(0,0): 5, (0,1): 1, (1,0): 1, (1,1): 1} (model 13)\n",
    "\n",
    "#analysis: the above max parameters result in a very slightly higher F1 score than the baseline\n",
    "# but with low precision and high recall and as such I believe the baseline (model 2, AKA unweighted) \n",
    "# is in fact the most robust model since it has precision roughly equal to recall roughly equal to F1\n",
    "# as well as the highest accuracy across all tests\n",
    "\n",
    "#final model parameters: k = 0.5, weights = {(0,0): 1, (0,1): 1, (1,0): 1, (1,1): 1} (model 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_1(model, val_data):\n",
    "    labels = []\n",
    "    for row in val_data.iterrows():\n",
    "        sentence = row[1][0]\n",
    "        labels += model.viterbi(sentence)\n",
    "    ids = [i for i in range(len(labels))]\n",
    "    df = pd.DataFrame({'idx': ids, 'label': labels}, columns = ['idx', 'label'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_model = HMM_Metaphor_Tagger(train_data)\n",
    "df = validate_model_1(hmm_model, val_data)\n",
    "df.to_csv('model_1_validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validate_model_1(hmm_model, test_data)\n",
    "df.to_csv('model_1_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: MaxEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxEnt_Metaphor_Tagger():\n",
    "    def __init__(self, train_data, size, k):\n",
    "        '''\n",
    "        train_data: dataframe of word features where the first column is the sentence, second column\n",
    "            is a string of a list of POS_Sequences, and third columnn is a string of a list of metaphor \n",
    "            label sequences (0: not metaphor, 1: metaphor)\n",
    "        '''\n",
    "        \n",
    "        self.size = size\n",
    "        self.k = k\n",
    "        self.emissions = {}\n",
    "        # Transform POS_Seq\n",
    "        self.pos_seqs_list_of_lists = [ast.literal_eval(pos_seq) for pos_seq in train_data['pos_seq']]\n",
    "        self.pos_seqs = []\n",
    "        for pos_seq in train_data['pos_seq']:\n",
    "            self.pos_seqs += ast.literal_eval(pos_seq)\n",
    "        self.pos_model = W2VTransformer(size=1, min_count=1, seed=1)\n",
    "        self.posvecs = np.array(self.pos_model.fit(self.pos_seqs_list_of_lists).transform(self.pos_seqs))\n",
    "        \n",
    "        # Transform Word Tokens\n",
    "        self.word_token_list_of_lists = [sentence.lower().split() for sentence in train_data['sentence']]\n",
    "        self.word_tokens = []\n",
    "        self.sample_indices = []\n",
    "        for i, sentence in enumerate(train_data['sentence']):\n",
    "            self.word_tokens += sentence.lower().split()\n",
    "            self.sample_indices += [i for x in range(len(sentence))]\n",
    "        \n",
    "        \n",
    "        self.word_model = W2VTransformer(size=self.size, min_count=1, seed=1)\n",
    "        self.wordvecs = np.array(self.word_model.fit(self.word_token_list_of_lists).transform(self.word_tokens))\n",
    "        \n",
    "        # Transform P(metaphor | word)\n",
    "        self.word_counts = Counter(self.word_tokens)\n",
    "        \n",
    "        #TF-IDF \n",
    "        self.tfidf_vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\S+|[0-9]|!|\\'m|\\.|\\'ll|:|\\'re|\\'s|,|i\\.e\\.|\\'ve|\\'d|\\(|\\)|a|i|n\\'t|;|\\?|\\\"|\\'|–\")\n",
    "        self.tfidf_vecs = self.tfidf_vectorizer.fit_transform([sentence.lower() for sentence in train_data['sentence']]).toarray()\n",
    "        \n",
    "        # Concatenate Feature Vectors\n",
    "        self.vecs = np.concatenate((self.wordvecs, self.posvecs), axis=1)\n",
    "        \n",
    "        feature_names = self.tfidf_vectorizer.get_feature_names()\n",
    "        \n",
    "        for i, word_vec in enumerate(self.vecs):\n",
    "            word = self.word_tokens[i]\n",
    "            sentence_index = self.sample_indices[i]\n",
    "            word_index = feature_names.index(word)\n",
    "            tfidf = self.tfidf_vecs[sentence_index][word_index]\n",
    "            word_vec += tfidf\n",
    "        \n",
    "        # Create Target Vector\n",
    "        self.label_seqs = []\n",
    "        for label_seq in train_data['label_seq']:\n",
    "            self.label_seqs += ast.literal_eval(label_seq)\n",
    "        \n",
    "        # Train MaxEnt classifier\n",
    "        self.classifier = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').\\\n",
    "            fit(self.vecs, self.label_seqs)\n",
    "        \n",
    "        # Get tag bigrams for transition probabilities\n",
    "        self.tag_counts = Counter()\n",
    "        self.tag_bigrams = {}\n",
    "        for row in train_data.iterrows():\n",
    "            \n",
    "            #preprocess: add start characters and labels for computing initial probabilities\n",
    "            # and convert strings to lists and downcase sentences\n",
    "            tags_string = row[1][2]\n",
    "            tags = ast.literal_eval(tags_string)\n",
    "            tags.insert(0, '<START>')\n",
    "            sentence = row[1][0].lower().split()\n",
    "            sentence.insert(0, '<s>')\n",
    "            \n",
    "            #get label bigram counts -- (0,0), (0,1), (1,0), (1,1), ('<START>',0), ('<START>',1)\n",
    "            for t in range(1, len(tags)):\n",
    "                tag_bigram = (tags[t-1], tags[t])\n",
    "                if tag_bigram not in self.tag_bigrams:\n",
    "                    self.tag_bigrams[tag_bigram] = 1\n",
    "                else:\n",
    "                    self.tag_bigrams[tag_bigram] += 1\n",
    "                    \n",
    "            #get individual tag counts\n",
    "            self.tag_counts.update(tags)\n",
    "            \n",
    "            #get emission counts\n",
    "            for i, word in enumerate(sentence):\n",
    "                if word not in self.emissions:\n",
    "                    self.emissions[word] = {tags[i] : 1}\n",
    "                else:\n",
    "                    if tags[i] not in self.emissions[word]:\n",
    "                        self.emissions[word][tags[i]] = 1\n",
    "                    else:\n",
    "                        self.emissions[word][tags[i]] += 1\n",
    "               \n",
    "            self.metaphor_frequencies = np.zeros(shape=(len(self.word_tokens), 1))\n",
    "            for i, word in enumerate(sentence):\n",
    "                if sentence[i] in self.emissions:\n",
    "                    self.metaphor_frequencies[i] = self.emissions[sentence[0]].get(1, self.k) / self.word_counts.get(word, 1)\n",
    "                else:\n",
    "                    self.metaphor_frequencies[i] = self.k / self.word_counts.get(word, 1)\n",
    "                \n",
    "        self.vecs = np.concatenate((self.vecs, self.metaphor_frequencies), axis=1)\n",
    "\n",
    "    def transform_sentence(self, sentence, pos_sequence):\n",
    "        wordvecs = np.zeros(shape=(len(sentence.split()), self.size))\n",
    "        posvecs = np.zeros(shape=(len(pos_sequence), 1))\n",
    "        \n",
    "        for i, word in enumerate(sentence.lower().split()):\n",
    "            if word in self.word_tokens:\n",
    "                wordvecs[i] = self.word_model.transform(word)\n",
    "            else:\n",
    "                replacement_word = self.replace_unknown_word(word) \n",
    "                if replacement_word == '':\n",
    "                    wordvecs[i] = np.array([None for i in range(0, self.size)])\n",
    "                else:\n",
    "                    wordvecs[i] = self.word_model.transform(replacement_word)\n",
    "\n",
    "\n",
    "            posvecs[i] = self.pos_model.transform(pos_sequence[i])\n",
    "        \n",
    "        \n",
    "        return np.concatenate((wordvecs, posvecs), axis=1)\n",
    "    \n",
    "    def predict_log_proba(self, vectors):\n",
    "        return self.classifier.predict_log_proba(vectors)\n",
    "    \n",
    "    def predict(self, vectors):\n",
    "        return self.classifier.predict(vectors)\n",
    "    \n",
    "    def replace_unknown_word(self, unknown_word):\n",
    "        max_combo = ['', 0]\n",
    "        synsets_names = []\n",
    "        synsets =  wn.synsets(unknown_word)\n",
    "        if len(synsets) == 0:\n",
    "            return max_combo[0]\n",
    "        for synset in synsets:\n",
    "            synsets_names.append(re.findall('^(\\S*)\\.+', synset.name())[0])\n",
    "        \n",
    "        synsets = wn.synsets(unknown_word)\n",
    "\n",
    "        for i, synset in enumerate(synsets):\n",
    "            if synsets_names[i] in self.word_tokens:\n",
    "                similarity = synset.path_similarity(wn.synsets(unknown_word)[0])\n",
    "                if similarity is not None and similarity > max_combo[1]:\n",
    "                    max_combo[0] = synsets_names[i]\n",
    "                    max_combo[1] = similarity\n",
    "        return max_combo[0]\n",
    "    \n",
    "    def viterbi(self, sentence, pos_seq):\n",
    "        feature_vectors = self.transform_sentence(sentence, pos_seq)\n",
    "        sentence = sentence.lower().split()\n",
    "        previous_log_scores = []\n",
    "        backpointers = []\n",
    "        tags = list(self.tag_counts)\n",
    "\n",
    "        #initialization\n",
    "        for t in range(1, len(tags)):\n",
    "            tag = tags[t]\n",
    "            \n",
    "            #TRANSITION PROB\n",
    "            initial_transition_prob = self.tag_bigrams[('<START>', tag)] / self.tag_counts['<START>']\n",
    "            \n",
    "            #EMISSION PROB\n",
    "            if sentence[0] in self.emissions:\n",
    "                initial_emission_prob = self.emissions[sentence[0]].get(tag, self.k) / self.tag_counts[tag]\n",
    "            else:\n",
    "                initial_emission_prob = self.k / self.tag_counts[tag]\n",
    "            \n",
    "            #TRAINING PROB\n",
    "            if np.isnan(feature_vectors[0]).any():\n",
    "                initial_maxent_log_prob = initial_transition_prob\n",
    "            else:\n",
    "                initial_maxent_log_prob = self.classifier.predict_log_proba([feature_vectors[0]])[0][t-1]\n",
    "            \n",
    "            previous_log_scores.append(math.log(initial_emission_prob) + initial_maxent_log_prob)\n",
    "        \n",
    "        #iteration\n",
    "        #w is index of current word\n",
    "        for w in range(1, feature_vectors.shape[0]):\n",
    "            log_scores = [None, None]\n",
    "            w_backpointers = []\n",
    "            max_log_score_final = (float('-inf'), None)\n",
    "            \n",
    "            #t is index of current tag\n",
    "            for t in range(1, len(tags)):\n",
    "                \n",
    "                t_backpointer = None\n",
    "                max_log_score = (float('-inf'), None)\n",
    "\n",
    "                #j is index of previous tag\n",
    "                for j in range(1, len(tags)):\n",
    "                    \n",
    "                    #TRANSITION\n",
    "                    transition_prob = self.tag_bigrams[(tags[j], tags[t])] / self.tag_counts[tags[j]]\n",
    "                    \n",
    "                    #EMISSION PROB\n",
    "                    if sentence[w] in self.emissions:\n",
    "                        emission_prob = self.emissions[sentence[w]].get(tags[t], self.k) / self.tag_counts[tags[t]]\n",
    "                    else:\n",
    "                        emission_prob = self.k / self.tag_counts[tags[t]]\n",
    "                    \n",
    "                    #TRAINING PROB\n",
    "                    if np.isnan(feature_vectors[w]).any():\n",
    "                        maxent_log_prob = transition_prob\n",
    "                    else:\n",
    "                        maxent_log_prob = self.classifier.predict_log_proba([feature_vectors[w]])[0][t-1]\n",
    "                    \n",
    "                    log_score = previous_log_scores[j-1] + math.log(emission_prob) + maxent_log_prob \n",
    "                    if log_score > max_log_score[0]:\n",
    "                        max_log_score = (log_score, j)\n",
    "                        t_backpointer = j\n",
    "                        \n",
    "                    if max_log_score[0] > max_log_score_final[0]:\n",
    "                        max_log_score_final = max_log_score\n",
    "                    \n",
    "                \n",
    "                log_scores[t-1] = max_log_score[0]\n",
    "                w_backpointers.append(t_backpointer)\n",
    "                \n",
    "            previous_log_scores = log_scores\n",
    "            backpointers.insert(0, w_backpointers)\n",
    "        \n",
    "        #backtracking\n",
    "        max_index = previous_log_scores.index(max(previous_log_scores)) + 1\n",
    "        output = [tags[max_index]]\n",
    "    \n",
    "        if feature_vectors.shape[0] == 1:\n",
    "            return output\n",
    "        \n",
    "        max_index = max_log_score_final[1]\n",
    "        for bptrs in backpointers:\n",
    "            max_index = bptrs[max_index-1]\n",
    "            output.insert(0, tags[max_index])\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_2(model, data):\n",
    "    labels = []\n",
    "    for row in data.iterrows():\n",
    "        sentence = row[1][0]\n",
    "        labels += model.viterbi(sentence, ast.literal_eval(row[1][1]))\n",
    "    ids = [i for i in range(len(labels))]\n",
    "    df = pd.DataFrame({'idx': ids, 'label': labels}, columns = ['idx', 'label'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxent_model = MaxEnt_Metaphor_Tagger(train_data, 15, 0.01)\n",
    "df = validate_model_2(maxent_model, val_data)\n",
    "df.to_csv('model_2_validation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxent_model = MaxEnt_Metaphor_Tagger(train_data, 15, 0.01)\n",
    "df = validate_model_2(maxent_model, val_data)\n",
    "df.to_csv('model_2_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
