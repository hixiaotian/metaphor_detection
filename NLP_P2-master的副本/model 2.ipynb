{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from gensim.sklearn_api import W2VTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data_release/train.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Seq Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_seqs_list_of_lists = [pos_seq.split() for pos_seq in train_data['pos_seq']]\n",
    "\n",
    "pos_seqs = []\n",
    "for pos_seq in train_data['pos_seq']:\n",
    "    pos_seqs += pos_seq.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116622, 1)\n"
     ]
    }
   ],
   "source": [
    "pos_model = W2VTransformer(size=1, min_count=1, seed=1)\n",
    "# What is the vector representation of the word 'graph'?\n",
    "posvecs = pos_model.fit(pos_seqs_list_of_lists).transform(pos_seqs)\n",
    "posvecs = np.array(posvecs)\n",
    "print(posvecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word token transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_token_list_of_lists = [sentence.lower().split() for sentence in train_data['sentence']]\n",
    "\n",
    "word_tokens = []\n",
    "for sentence in train_data['sentence']:\n",
    "    word_tokens += sentence.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116622, 10)\n"
     ]
    }
   ],
   "source": [
    "word_model = W2VTransformer(size=10, min_count=1, seed=1)\n",
    "# What is the vector representation of the word 'graph'?\n",
    "wordvecs = word_model.fit(word_token_list_of_lists).transform(word_tokens)\n",
    "wordvecs = np.array(wordvecs)\n",
    "print(wordvecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116622, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs = np.concatenate((wordvecs, posvecs), axis=1)\n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_seqs = []\n",
    "for label_seq in train_data['label_seq']:\n",
    "    label_seqs += ast.literal_eval(label_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(vecs, label_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv('./data_release/val.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = clf.predict_log_proba(vecs)\n",
    "y = clf.predict(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxEnt_Metaphor_Tagger():\n",
    "    def __init__(self, train_data, size, k):\n",
    "        '''\n",
    "        train_data: dataframe of word features where the first column is the sentence, second column\n",
    "            is a string of a list of POS_Sequences, and third columnn is a string of a list of metaphor \n",
    "            label sequences (0: not metaphor, 1: metaphor)\n",
    "        '''\n",
    "        \n",
    "        self.size = size\n",
    "        self.k = k\n",
    "        self.emissions = {}\n",
    "        # Transform POS_Seq\n",
    "        self.pos_seqs_list_of_lists = [ast.literal_eval(pos_seq) for pos_seq in train_data['pos_seq']]\n",
    "        self.pos_seqs = []\n",
    "        for pos_seq in train_data['pos_seq']:\n",
    "            self.pos_seqs += ast.literal_eval(pos_seq)\n",
    "        self.pos_model = W2VTransformer(size=1, min_count=1, seed=1)\n",
    "        self.posvecs = np.array(self.pos_model.fit(self.pos_seqs_list_of_lists).transform(self.pos_seqs))\n",
    "        \n",
    "        # Transform Word Tokens\n",
    "        self.word_token_list_of_lists = [sentence.lower().split() for sentence in train_data['sentence']]\n",
    "        self.word_tokens = []\n",
    "        self.sample_indices = []\n",
    "        for i, sentence in enumerate(train_data['sentence']):\n",
    "            self.word_tokens += sentence.lower().split()\n",
    "            self.sample_indices += [i for x in range(len(sentence))]\n",
    "        \n",
    "        \n",
    "        self.word_model = W2VTransformer(size=self.size, min_count=1, seed=1)\n",
    "        self.wordvecs = np.array(self.word_model.fit(self.word_token_list_of_lists).transform(self.word_tokens))\n",
    "        \n",
    "        # Transform P(metaphor | word)\n",
    "        self.word_counts = Counter(self.word_tokens)\n",
    "        \n",
    "        #TF-IDF \n",
    "        self.tfidf_vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\S+|[0-9]|!|\\'m|\\.|\\'ll|:|\\'re|\\'s|,|i\\.e\\.|\\'ve|\\'d|\\(|\\)|a|i|n\\'t|;|\\?|\\\"|\\'|â€“\")\n",
    "        self.tfidf_vecs = self.tfidf_vectorizer.fit_transform([sentence.lower() for sentence in train_data['sentence']]).toarray()\n",
    "        \n",
    "        # Concatenate Feature Vectors\n",
    "        self.vecs = np.concatenate((self.wordvecs, self.posvecs), axis=1)\n",
    "        \n",
    "        feature_names = self.tfidf_vectorizer.get_feature_names()\n",
    "        \n",
    "        for i, word_vec in enumerate(self.vecs):\n",
    "            word = self.word_tokens[i]\n",
    "            sentence_index = self.sample_indices[i]\n",
    "            word_index = feature_names.index(word)\n",
    "            tfidf = self.tfidf_vecs[sentence_index][word_index]\n",
    "            word_vec += tfidf\n",
    "        \n",
    "        # Create Target Vector\n",
    "        self.label_seqs = []\n",
    "        for label_seq in train_data['label_seq']:\n",
    "            self.label_seqs += ast.literal_eval(label_seq)\n",
    "        \n",
    "        # Train MaxEnt classifier\n",
    "        self.classifier = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').\\\n",
    "            fit(self.vecs, self.label_seqs)\n",
    "        \n",
    "        # Get tag bigrams for transition probabilities\n",
    "        self.tag_counts = Counter()\n",
    "        self.tag_bigrams = {}\n",
    "        for row in train_data.iterrows():\n",
    "            \n",
    "            #preprocess: add start characters and labels for computing initial probabilities\n",
    "            # and convert strings to lists and downcase sentences\n",
    "            tags_string = row[1][2]\n",
    "            tags = ast.literal_eval(tags_string)\n",
    "            tags.insert(0, '<START>')\n",
    "            sentence = row[1][0].lower().split()\n",
    "            sentence.insert(0, '<s>')\n",
    "            \n",
    "            #get label bigram counts -- (0,0), (0,1), (1,0), (1,1), ('<START>',0), ('<START>',1)\n",
    "            for t in range(1, len(tags)):\n",
    "                tag_bigram = (tags[t-1], tags[t])\n",
    "                if tag_bigram not in self.tag_bigrams:\n",
    "                    self.tag_bigrams[tag_bigram] = 1\n",
    "                else:\n",
    "                    self.tag_bigrams[tag_bigram] += 1\n",
    "                    \n",
    "            #get individual tag counts\n",
    "            self.tag_counts.update(tags)\n",
    "            \n",
    "            #get emission counts\n",
    "            for i, word in enumerate(sentence):\n",
    "                if word not in self.emissions:\n",
    "                    self.emissions[word] = {tags[i] : 1}\n",
    "                else:\n",
    "                    if tags[i] not in self.emissions[word]:\n",
    "                        self.emissions[word][tags[i]] = 1\n",
    "                    else:\n",
    "                        self.emissions[word][tags[i]] += 1\n",
    "               \n",
    "            self.metaphor_frequencies = np.zeros(shape=(len(self.word_tokens), 1))\n",
    "            for i, word in enumerate(sentence):\n",
    "                if sentence[i] in self.emissions:\n",
    "                    self.metaphor_frequencies[i] = self.emissions[sentence[0]].get(1, self.k) / self.word_counts.get(word, 1)\n",
    "                else:\n",
    "                    self.metaphor_frequencies[i] = self.k / self.word_counts.get(word, 1)\n",
    "                \n",
    "        self.vecs = np.concatenate((self.vecs, self.metaphor_frequencies), axis=1)\n",
    "\n",
    "    def transform_sentence(self, sentence, pos_sequence):\n",
    "        wordvecs = np.zeros(shape=(len(sentence.split()), self.size))\n",
    "        posvecs = np.zeros(shape=(len(pos_sequence), 1))\n",
    "        \n",
    "        for i, word in enumerate(sentence.lower().split()):\n",
    "            if word in self.word_tokens:\n",
    "                wordvecs[i] = self.word_model.transform(word)\n",
    "            else:\n",
    "                replacement_word = self.replace_unknown_word(word) \n",
    "                if replacement_word == '':\n",
    "                    wordvecs[i] = np.array([None for i in range(0, self.size)])\n",
    "                else:\n",
    "                    wordvecs[i] = self.word_model.transform(replacement_word)\n",
    "\n",
    "\n",
    "            posvecs[i] = self.pos_model.transform(pos_sequence[i])\n",
    "        \n",
    "        \n",
    "        return np.concatenate((wordvecs, posvecs), axis=1)\n",
    "    \n",
    "    def predict_log_proba(self, vectors):\n",
    "        return self.classifier.predict_log_proba(vectors)\n",
    "    \n",
    "    def predict(self, vectors):\n",
    "        return self.classifier.predict(vectors)\n",
    "    \n",
    "    def replace_unknown_word(self, unknown_word):\n",
    "        max_combo = ['', 0]\n",
    "        synsets_names = []\n",
    "        synsets =  wn.synsets(unknown_word)\n",
    "        if len(synsets) == 0:\n",
    "            return max_combo[0]\n",
    "        for synset in synsets:\n",
    "            synsets_names.append(re.findall('^(\\S*)\\.+', synset.name())[0])\n",
    "        \n",
    "        synsets = wn.synsets(unknown_word)\n",
    "\n",
    "        for i, synset in enumerate(synsets):\n",
    "            if synsets_names[i] in self.word_tokens:\n",
    "                similarity = synset.path_similarity(wn.synsets(unknown_word)[0])\n",
    "                if similarity is not None and similarity > max_combo[1]:\n",
    "                    max_combo[0] = synsets_names[i]\n",
    "                    max_combo[1] = similarity\n",
    "        return max_combo[0]\n",
    "    \n",
    "    def viterbi(self, sentence, pos_seq):\n",
    "        feature_vectors = self.transform_sentence(sentence, pos_seq)\n",
    "        sentence = sentence.lower().split()\n",
    "        previous_log_scores = []\n",
    "        backpointers = []\n",
    "        tags = list(self.tag_counts)\n",
    "\n",
    "        #initialization\n",
    "        for t in range(1, len(tags)):\n",
    "            tag = tags[t]\n",
    "            \n",
    "            #TRANSITION PROB\n",
    "            initial_transition_prob = self.tag_bigrams[('<START>', tag)] / self.tag_counts['<START>']\n",
    "            \n",
    "            #EMISSION PROB\n",
    "            if sentence[0] in self.emissions:\n",
    "                initial_emission_prob = self.emissions[sentence[0]].get(tag, self.k) / self.tag_counts[tag]\n",
    "            else:\n",
    "                initial_emission_prob = self.k / self.tag_counts[tag]\n",
    "            \n",
    "            #TRAINING PROB\n",
    "            if np.isnan(feature_vectors[0]).any():\n",
    "                initial_maxent_log_prob = initial_transition_prob\n",
    "            else:\n",
    "                initial_maxent_log_prob = self.classifier.predict_log_proba([feature_vectors[0]])[0][t-1]\n",
    "            \n",
    "            previous_log_scores.append(math.log(initial_emission_prob) + initial_maxent_log_prob)\n",
    "        \n",
    "        #iteration\n",
    "        #w is index of current word\n",
    "        for w in range(1, feature_vectors.shape[0]):\n",
    "            log_scores = [None, None]\n",
    "            w_backpointers = []\n",
    "            max_log_score_final = (float('-inf'), None)\n",
    "            \n",
    "            #t is index of current tag\n",
    "            for t in range(1, len(tags)):\n",
    "                \n",
    "                t_backpointer = None\n",
    "                max_log_score = (float('-inf'), None)\n",
    "\n",
    "                #j is index of previous tag\n",
    "                for j in range(1, len(tags)):\n",
    "                    \n",
    "                    #TRANSITION\n",
    "                    transition_prob = self.tag_bigrams[(tags[j], tags[t])] / self.tag_counts[tags[j]]\n",
    "                    \n",
    "                    #EMISSION PROB\n",
    "                    if sentence[w] in self.emissions:\n",
    "                        emission_prob = self.emissions[sentence[w]].get(tags[t], self.k) / self.tag_counts[tags[t]]\n",
    "                    else:\n",
    "                        emission_prob = self.k / self.tag_counts[tags[t]]\n",
    "                    \n",
    "                    #TRAINING PROB\n",
    "                    if np.isnan(feature_vectors[w]).any():\n",
    "                        maxent_log_prob = transition_prob\n",
    "                    else:\n",
    "                        maxent_log_prob = self.classifier.predict_log_proba([feature_vectors[w]])[0][t-1]\n",
    "                    \n",
    "                    log_score = previous_log_scores[j-1] + math.log(emission_prob) + maxent_log_prob \n",
    "                    if log_score > max_log_score[0]:\n",
    "                        max_log_score = (log_score, j)\n",
    "                        t_backpointer = j\n",
    "                        \n",
    "                    if max_log_score[0] > max_log_score_final[0]:\n",
    "                        max_log_score_final = max_log_score\n",
    "                    \n",
    "                \n",
    "                log_scores[t-1] = max_log_score[0]\n",
    "                w_backpointers.append(t_backpointer)\n",
    "                \n",
    "            previous_log_scores = log_scores\n",
    "            backpointers.insert(0, w_backpointers)\n",
    "        \n",
    "        #backtracking\n",
    "        max_index = previous_log_scores.index(max(previous_log_scores)) + 1\n",
    "        output = [tags[max_index]]\n",
    "    \n",
    "        if feature_vectors.shape[0] == 1:\n",
    "            return output\n",
    "        \n",
    "        max_index = max_log_score_final[1]\n",
    "        for bptrs in backpointers:\n",
    "            max_index = bptrs[max_index-1]\n",
    "            output.insert(0, tags[max_index])\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxent = MaxEnt_Metaphor_Tagger(train_data, 12, 0.01)\n",
    "\n",
    "val_data = pd.read_csv('./data_release/val.csv', encoding='latin-1')\n",
    "def validate(model, data):\n",
    "    labels = []\n",
    "    for row in data.iterrows():\n",
    "        sentence = row[1][0]\n",
    "        labels += model.viterbi(sentence, ast.literal_eval(row[1][1]))\n",
    "    ids = [i for i in range(len(labels))]\n",
    "    df = pd.DataFrame({'idx': ids, 'label': labels}, columns = ['idx', 'label'])\n",
    "    return df\n",
    "df = validate(maxent, val_data)\n",
    "df.to_csv('model_2_validation_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = wn.synset('face-lift.v.01').name()\n",
    "re.findall('^(\\w*)\\.+', temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'he continued , hackles rising .'\n",
    "test_pos_seq = ['PRON', 'VERB', 'PUNCT', 'NOUN', 'VERB', 'PUNCT']\n",
    "maxent.viterbi(test_sentence, test_pos_seq)\n",
    "maxent = MaxEnt_Metaphor_Tagger(train_data, 15, 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
